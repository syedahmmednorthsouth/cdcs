{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58911b4e-eca3-4a62-a61c-218bca3e72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lbl2vec\n",
      "  Using cached lbl2vec-1.0.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from lbl2vec) (1.2.0)\n",
      "Collecting gensim>=4.0.1\n",
      "  Using cached gensim-4.3.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "Collecting transformers>=4.24.0\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from lbl2vec) (1.23.5)\n",
      "Collecting torch>=1.13\n",
      "  Using cached torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from lbl2vec) (5.9.4)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from lbl2vec) (1.5.3)\n",
      "Collecting syntok>=1.4.4\n",
      "  Using cached syntok-1.4.4-py3-none-any.whl (24 kB)\n",
      "Collecting sentence-transformers>=2.2.2\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ray>=2.1.0\n",
      "  Using cached ray-2.2.0-cp39-cp39-manylinux2014_x86_64.whl (57.4 MB)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Using cached FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from gensim>=4.0.1->lbl2vec) (1.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from pandas>=1.3.0->lbl2vec) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from pandas>=1.3.0->lbl2vec) (2022.7.1)\n",
      "Collecting click>=7.0\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting aiosignal\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from ray>=2.1.0->lbl2vec) (22.2.0)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3\n",
      "  Using cached protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "Requirement already satisfied: requests in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from ray>=2.1.0->lbl2vec) (2.28.2)\n",
      "Collecting virtualenv>=20.0.24\n",
      "  Using cached virtualenv-20.17.1-py3-none-any.whl (8.8 MB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Using cached msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: pyyaml in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from ray>=2.1.0->lbl2vec) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from ray>=2.1.0->lbl2vec) (4.17.3)\n",
      "Collecting grpcio>=1.32.0\n",
      "  Using cached grpcio-1.51.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Collecting frozenlist\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from scikit-learn>=0.24.2->lbl2vec) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from scikit-learn>=0.24.2->lbl2vec) (2.2.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.14.1-cp39-cp39-manylinux1_x86_64.whl (24.2 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "Collecting regex>2016\n",
      "  Using cached regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m974.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13->lbl2vec) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13->lbl2vec) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from transformers>=4.24.0->lbl2vec) (23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m947.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:01\u001b[0m00:01\u001b[0m[91m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyfume\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m234.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.3.0->lbl2vec) (1.16.0)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray>=2.1.0->lbl2vec) (2.6.2)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Using cached distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from jsonschema->ray>=2.1.0->lbl2vec) (0.19.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from requests->ray>=2.1.0->lbl2vec) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from requests->ray>=2.1.0->lbl2vec) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from requests->ray>=2.1.0->lbl2vec) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from requests->ray>=2.1.0->lbl2vec) (2022.12.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from torchvision->sentence-transformers>=2.2.2->lbl2vec) (9.3.0)\n",
      "Collecting simpful\n",
      "  Downloading simpful-2.9.0-py3-none-any.whl (30 kB)\n",
      "Collecting fst-pso\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting miniful\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence-transformers, fst-pso, miniful\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=70aff58a19a80344111125edefd6364b8d10bcaeda112ea34175af2c8e441fe4\n",
      "  Stored in directory: /home/syed/.cache/pip/wheels/4b/68/65/aba8be86302d9988b832f5e1f3417a87e4a868d396e4329f0a\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20431 sha256=f01dd81be8542f87aad273b3102d2710faf855f1138a6550a4726a96a94e9e48\n",
      "  Stored in directory: /home/syed/.cache/pip/wheels/d7/1e/9c/3c352867b1bb1ddba0e51149a0d06c1e50c1c9b1c5ebcac787\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3514 sha256=6919ec8647c644ba224a98fb20d565d74391d63fd185ca4216510a56d18878ba\n",
      "  Stored in directory: /home/syed/.cache/pip/wheels/37/45/a2/db878712db97aaab6f574d750789bc6c99e3b706af8af6b936\n",
      "Successfully built sentence-transformers fst-pso miniful\n",
      "Installing collected packages: tokenizers, sentencepiece, msgpack, distlib, typing-extensions, tqdm, smart-open, regex, protobuf, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, grpcio, frozenlist, filelock, click, virtualenv, syntok, simpful, nvidia-cudnn-cu11, nltk, miniful, huggingface-hub, aiosignal, transformers, torch, ray, fst-pso, torchvision, pyfume, sentence-transformers, FuzzyTM, gensim, lbl2vec\n",
      "Successfully installed FuzzyTM-2.0.5 aiosignal-1.3.1 click-8.1.3 distlib-0.3.6 filelock-3.9.0 frozenlist-1.3.3 fst-pso-1.8.1 gensim-4.3.0 grpcio-1.51.1 huggingface-hub-0.12.0 lbl2vec-1.0.2 miniful-0.0.6 msgpack-1.0.4 nltk-3.8.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 protobuf-4.21.12 pyfume-0.2.25 ray-2.2.0 regex-2022.10.31 sentence-transformers-2.2.2 sentencepiece-0.1.97 simpful-2.9.0 smart-open-6.3.0 syntok-1.4.4 tokenizers-0.13.2 torch-1.13.1 torchvision-0.14.1 tqdm-4.64.1 transformers-4.26.0 typing-extensions-4.4.0 virtualenv-20.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lbl2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd873d0f-d581-41e5-a109-dadeadc07056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8fdca4-369e-4aa3-90ec-8b8538a8f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/syed/miniconda3/envs/py39/lib/python3.9/site-packages (from scikit-learn) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00934dc9-3a12-4b5f-8ed5-f028d17cc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90db404c-4896-46ba-8d25-0fdc2a7fe457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "\n",
    "# parse data to pandas DataFrames\n",
    "newsgroup_test = pd.DataFrame({'article':test.data, 'class_index':test.target})\n",
    "newsgroup_train = pd.DataFrame({'article':train.data, 'class_index':train.target})\n",
    "\n",
    "# load labels with keywords\n",
    "labels = pd.read_csv('./keywords/20newsgroups_keywords.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5604115d-9d5d-4269-a685-59f6ac93120d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>class_index</th>\n",
       "      <th>data_set_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: stimpy@dev-null.phys.psu.edu (Gregory Na...</td>\n",
       "      <td>10</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: kennejs@a.cs.okstate.edu (KENNEDY JAMES ...</td>\n",
       "      <td>16</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: perky@acs.bu.edu (Melissa Sherrin)\\nSubj...</td>\n",
       "      <td>14</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: evansmp@uhura.aston.ac.uk (Mark Evans)\\n...</td>\n",
       "      <td>18</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: sxs@extol.Convergent.Com (S. Sridhar)\\nS...</td>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>From: fennell@well.sf.ca.us (Michael Daniel Fe...</td>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7528</th>\n",
       "      <td>From: tony@morgan.demon.co.uk (Tony Kidson)\\nS...</td>\n",
       "      <td>8</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>From: sadams@eis.calstate.edu (Steven Adams)\\n...</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>From: jtobias@cs.tamu.edu (Jason T Tobias)\\nSu...</td>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7531</th>\n",
       "      <td>From: jgoss@gaia.torolab.ibm.com (Jeff Goss)\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7532 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  class_index  \\\n",
       "0     From: stimpy@dev-null.phys.psu.edu (Gregory Na...           10   \n",
       "1     From: kennejs@a.cs.okstate.edu (KENNEDY JAMES ...           16   \n",
       "2     From: perky@acs.bu.edu (Melissa Sherrin)\\nSubj...           14   \n",
       "3     From: evansmp@uhura.aston.ac.uk (Mark Evans)\\n...           18   \n",
       "4     From: sxs@extol.Convergent.Com (S. Sridhar)\\nS...            5   \n",
       "...                                                 ...          ...   \n",
       "7527  From: fennell@well.sf.ca.us (Michael Daniel Fe...           12   \n",
       "7528  From: tony@morgan.demon.co.uk (Tony Kidson)\\nS...            8   \n",
       "7529  From: sadams@eis.calstate.edu (Steven Adams)\\n...            4   \n",
       "7530  From: jtobias@cs.tamu.edu (Jason T Tobias)\\nSu...            6   \n",
       "7531  From: jgoss@gaia.torolab.ibm.com (Jeff Goss)\\n...            7   \n",
       "\n",
       "     data_set_type  \n",
       "0             test  \n",
       "1             test  \n",
       "2             test  \n",
       "3             test  \n",
       "4             test  \n",
       "...            ...  \n",
       "7527          test  \n",
       "7528          test  \n",
       "7529          test  \n",
       "7530          test  \n",
       "7531          test  \n",
       "\n",
       "[7532 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e15eba1e-56c4-4443-8d14-f091a69ec90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_index          class_name               keywords  number_of_keywords\n",
      "0            8     rec.motorcycles    [bikes, motorcycle]                   2\n",
      "1            9  rec.sport.baseball             [baseball]                   1\n",
      "2           10    rec.sport.hockey               [hockey]                   1\n",
      "3           11           sci.crypt  [encryption, privacy]                   2\n"
     ]
    }
   ],
   "source": [
    "# split keywords by separator and save them as array\n",
    "labels['keywords'] = labels['keywords'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# convert description keywords to lowercase\n",
    "labels['keywords'] = labels['keywords'].apply(lambda description_keywords: [keyword.lower() for keyword in description_keywords])\n",
    "\n",
    "# get number of keywords for each class\n",
    "labels['number_of_keywords'] = labels['keywords'].apply(lambda row: len(row))\n",
    "\n",
    "# lets check our keywords\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd8206b-4503-4dc8-9ea1-b9aa395c89c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             article  class_index  \\\n",
      "0  From: cubbie@garnet.berkeley.edu (            ...            9   \n",
      "1  From: crypt-comments@math.ncsu.edu\\nSubject: C...           11   \n",
      "2  From: george@minster.york.ac.uk\\nSubject: Non-...           11   \n",
      "3  From: williac@govonca.gov.on.ca (Chris William...           10   \n",
      "4  From: ayari@judikael.loria.fr (Ayari Iskander)...           10   \n",
      "\n",
      "  data_set_type                                        tagged_docs doc_key  \\\n",
      "0         train  ([from, cubbie, garnet, berkeley, edu, subject...       0   \n",
      "1         train  ([from, crypt, comments, math, ncsu, edu, subj...       2   \n",
      "2         train  ([from, george, minster, york, ac, uk, subject...      11   \n",
      "3         train  ([from, williac, govonca, gov, on, ca, chris, ...      12   \n",
      "4         train  ([from, ayari, judikael, loria, fr, ayari, isk...      15   \n",
      "\n",
      "           class_name               keywords  number_of_keywords  \n",
      "0  rec.sport.baseball             [baseball]                   1  \n",
      "1           sci.crypt  [encryption, privacy]                   2  \n",
      "2           sci.crypt  [encryption, privacy]                   2  \n",
      "3    rec.sport.hockey               [hockey]                   1  \n",
      "4    rec.sport.hockey               [hockey]                   1  \n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# doc: document text string\n",
    "# returns tokenized document\n",
    "# strip_tags removes meta tags from the text\n",
    "# simple preprocess converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long \n",
    "# simple preprocess also removes numerical values as well as punktuation characters\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "# add data set type column\n",
    "newsgroup_train['data_set_type'] = 'train'\n",
    "newsgroup_test['data_set_type'] = 'test'\n",
    "\n",
    "# concat train and test data\n",
    "newsgroup_full_corpus = pd.concat([newsgroup_train,newsgroup_test]).reset_index(drop=True)\n",
    "\n",
    "# reduce dataset to only articles that belong to classes where we defined our keywords\n",
    "newsgroup_full_corpus = newsgroup_full_corpus[newsgroup_full_corpus['class_index'].isin(list(labels['class_index']))]\n",
    "\n",
    "# tokenize and tag documents for Lbl2Vec training\n",
    "newsgroup_full_corpus['tagged_docs'] = newsgroup_full_corpus.apply(lambda row: TaggedDocument(tokenize(row['article']), [str(row.name)]), axis=1)\n",
    "\n",
    "# add doc_key column\n",
    "newsgroup_full_corpus['doc_key'] = newsgroup_full_corpus.index.astype(str)\n",
    "\n",
    "# add class_name column\n",
    "newsgroup_full_corpus = newsgroup_full_corpus.merge(labels, left_on='class_index', right_on='class_index', how='left')\n",
    "\n",
    "print(newsgroup_full_corpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5692d26-ab21-4617-ad6a-795def643d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 02:32:18,472 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2023-01-31 02:32:18,472 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2023-01-31 02:32:26,286 - Lbl2Vec - INFO - Train label embeddings\n",
      "2023-01-31 02:32:26,286 - Lbl2Vec - INFO - Train label embeddings\n"
     ]
    }
   ],
   "source": [
    "from lbl2vec import Lbl2Vec\n",
    "\n",
    "# init model with parameters\n",
    "Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels.keywords), tagged_documents=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type'] == 'train'], label_names=list(labels.class_name), similarity_threshold=0.43, min_num_docs=100, epochs=10)\n",
    "\n",
    "# train model\n",
    "Lbl2Vec_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92a33f9b-c684-4a26-a13b-891f266901fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 02:31:24,251 - Lbl2Vec - INFO - Get document embeddings from model\n",
      "2023-01-31 02:31:24,255 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.896234309623431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# predict similarity scores\n",
    "model_docs_lbl_similarities = Lbl2Vec_model.predict_model_docs()\n",
    "\n",
    "# merge DataFrames to compare the predicted and true category labels\n",
    "evaluation_train = model_docs_lbl_similarities.merge(newsgroup_full_corpus[newsgroup_full_corpus['data_set_type'] == 'train'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_train = evaluation_train['class_name']\n",
    "y_pred_train = evaluation_train['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_train, y_pred_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b43fa94-5536-4e0b-be4e-b626685b1467",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "keywords_list has to be an iterable list of lists with descriptive keywords of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlbl2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lbl2TransformerVec\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# init model using the default transformer-embedding model (\"sentence-transformers/all-MiniLM-L6-v2\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLbl2TransformerVec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNot Found\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNot Specified\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNor Found\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNot Found\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNot done.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNot found\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNot found.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mfit()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/lbl2vec/lbl2transformervec.py:116\u001b[0m, in \u001b[0;36mLbl2TransformerVec.__init__\u001b[0;34m(self, keywords_list, documents, transformer_model, label_names, similarity_threshold, similarity_threshold_offset, min_num_docs, max_num_docs, clean_outliers, workers, device, verbose)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# validate keywords_list\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keywords_list, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m keywords_list)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m keywords_list \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist])):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords_list has to be an iterable list of lists with descriptive keywords of type str\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# init labels DataFrame\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(label_names, keywords_list)), columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_keywords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: keywords_list has to be an iterable list of lists with descriptive keywords of type str"
     ]
    }
   ],
   "source": [
    "from lbl2vec import Lbl2TransformerVec\n",
    "\n",
    "# init model using the default transformer-embedding model (\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = Lbl2TransformerVec(keywords_list=[\"Not Found\",\"Not Specified\"], documents=[\"Nor Found\" ,\"Not Found\",\"Not done.\" , \"Not found\",\n",
    "\"Not found.\"])\n",
    "\n",
    "# train model\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a0382-99cf-4ce5-8dbf-b6e1c5e0be8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
